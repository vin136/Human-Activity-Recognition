{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Conv1D,BatchNormalization,MaxPooling1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "n_hidden = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 128, 9), (7352, 6))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 13,894\n",
      "Trainable params: 13,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden,recurrent_dropout= 0.3,return_sequences=True ,input_shape=(timesteps, input_dim)))\n",
    "model.add(LSTM(n_hidden,recurrent_dropout = 0.3)) #since we added another lstm we got to increase epochs..\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### THAT'S MORE THAN TWICE THE NUMBER OF MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here i wanted to share one idea...We could take the initial layer conv1d ...before passing onto rnn layer i remember trying this out with char rnn when i first heard about them.The intuition is simple:\n",
    "## -> say in this question we have 128 dimensional vectors laid out temporally ->7352 of them what if we take a simple a linear combination of them say with a kernel(3) this decreases the width....one advantage is now we could use larger rnn's ( decreased parameters due to the above operation.)Most importantly we expect the device recordings to vary smoothly -> extracting features and then passing on to rnn's for sequence information seems to be a good idea thought i haven't seen that widely used...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
    "# model.add(MaxPooling1D(mp_size))\n",
    "# model.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 13,894\n",
      "Trainable params: 13,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.7174 - acc: 0.6610 - val_loss: 0.7647 - val_acc: 0.6892\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6992 - acc: 0.6624 - val_loss: 0.7356 - val_acc: 0.6525\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.6851 - acc: 0.6707 - val_loss: 0.7282 - val_acc: 0.6366\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6604 - acc: 0.6834 - val_loss: 0.7104 - val_acc: 0.7082\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6389 - acc: 0.6963 - val_loss: 0.6922 - val_acc: 0.6491\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.6326 - acc: 0.7046 - val_loss: 0.6761 - val_acc: 0.6875\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.6147 - acc: 0.7140 - val_loss: 0.6142 - val_acc: 0.7153\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.5856 - acc: 0.7243 - val_loss: 0.5935 - val_acc: 0.7004\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.5705 - acc: 0.7458 - val_loss: 0.5726 - val_acc: 0.7163\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.5495 - acc: 0.7631 - val_loss: 0.5526 - val_acc: 0.7475\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 57s 8ms/step - loss: 0.5328 - acc: 0.7907 - val_loss: 0.5296 - val_acc: 0.8113\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 56s 8ms/step - loss: 0.4882 - acc: 0.8134 - val_loss: 0.4834 - val_acc: 0.8442\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.4567 - acc: 0.8373 - val_loss: 0.4578 - val_acc: 0.8361\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.4429 - acc: 0.8447 - val_loss: 0.4340 - val_acc: 0.8612\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.4195 - acc: 0.8598 - val_loss: 0.4058 - val_acc: 0.8714\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.3907 - acc: 0.8754 - val_loss: 0.3808 - val_acc: 0.8707\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.3648 - acc: 0.8821 - val_loss: 0.3957 - val_acc: 0.8660\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.3506 - acc: 0.8920 - val_loss: 0.3770 - val_acc: 0.8792\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3259 - acc: 0.8980 - val_loss: 0.4230 - val_acc: 0.8622\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3145 - acc: 0.9044 - val_loss: 0.3781 - val_acc: 0.8795\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2932 - acc: 0.9127 - val_loss: 0.3843 - val_acc: 0.8799\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2795 - acc: 0.9187 - val_loss: 0.4073 - val_acc: 0.8802\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2724 - acc: 0.9191 - val_loss: 0.4134 - val_acc: 0.8816\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.2679 - acc: 0.9215 - val_loss: 0.3981 - val_acc: 0.8789\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2452 - acc: 0.9227 - val_loss: 0.3845 - val_acc: 0.8846\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.2341 - acc: 0.9295 - val_loss: 0.3741 - val_acc: 0.8850\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.2287 - acc: 0.9312 - val_loss: 0.3874 - val_acc: 0.8894\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.2286 - acc: 0.9286 - val_loss: 0.3748 - val_acc: 0.8958\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 56s 8ms/step - loss: 0.2183 - acc: 0.9324 - val_loss: 0.3958 - val_acc: 0.8979\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.2121 - acc: 0.9343 - val_loss: 0.3997 - val_acc: 0.8914\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "model.save('HAR_BASIC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "mp_size = 2\n",
    "model2 = Sequential()\n",
    "n_conv = 80\n",
    "k_conv = 5\n",
    "n_lstm = 32\n",
    "drop_lstm = 0.3\n",
    "\n",
    "model2.add(Conv1D(n_conv, k_conv, activation='relu',input_shape = (timesteps, input_dim)))\n",
    "model2.add(MaxPooling1D(mp_size))\n",
    "model2.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm)))\n",
    "model2.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2.compile(optimizer='Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 124, 80)           3680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 62, 80)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                28928     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 32,998\n",
      "Trainable params: 32,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.3786 - acc: 0.4584 - val_loss: 0.9134 - val_acc: 0.5711\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.7295 - acc: 0.6721 - val_loss: 0.6984 - val_acc: 0.7153\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.5785 - acc: 0.7578 - val_loss: 0.5850 - val_acc: 0.7462\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.4534 - acc: 0.8270 - val_loss: 0.5206 - val_acc: 0.8001\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.3565 - acc: 0.8670 - val_loss: 0.4800 - val_acc: 0.8215\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2817 - acc: 0.8985 - val_loss: 0.4062 - val_acc: 0.8653\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.2390 - acc: 0.9184 - val_loss: 0.3855 - val_acc: 0.8697\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2042 - acc: 0.9293 - val_loss: 0.3632 - val_acc: 0.8826\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1907 - acc: 0.9310 - val_loss: 0.3573 - val_acc: 0.8778\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1740 - acc: 0.9395 - val_loss: 0.3665 - val_acc: 0.8829\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1598 - acc: 0.9438 - val_loss: 0.3525 - val_acc: 0.8802\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1502 - acc: 0.9463 - val_loss: 0.3800 - val_acc: 0.8809\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1504 - acc: 0.9415 - val_loss: 0.3128 - val_acc: 0.8887\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1444 - acc: 0.9482 - val_loss: 0.3308 - val_acc: 0.8836\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.1378 - acc: 0.9476 - val_loss: 0.3050 - val_acc: 0.8901\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 41s 6ms/step - loss: 0.1308 - acc: 0.9480 - val_loss: 0.2933 - val_acc: 0.8962\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.1323 - acc: 0.9457 - val_loss: 0.2992 - val_acc: 0.8951\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1255 - acc: 0.9510 - val_loss: 0.2922 - val_acc: 0.8897\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1227 - acc: 0.9494 - val_loss: 0.2995 - val_acc: 0.9016\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.1231 - acc: 0.9506 - val_loss: 0.2903 - val_acc: 0.8935\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 0.1180 - acc: 0.9509 - val_loss: 0.2982 - val_acc: 0.8975\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1186 - acc: 0.9521 - val_loss: 0.3083 - val_acc: 0.8890\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1179 - acc: 0.9517 - val_loss: 0.2878 - val_acc: 0.8880\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1129 - acc: 0.9536 - val_loss: 0.3044 - val_acc: 0.8928\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1140 - acc: 0.9544 - val_loss: 0.2590 - val_acc: 0.9002\n",
      "Epoch 26/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1142 - acc: 0.9548 - val_loss: 0.2724 - val_acc: 0.8918\n",
      "Epoch 27/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1088 - acc: 0.9550 - val_loss: 0.2849 - val_acc: 0.8914\n",
      "Epoch 28/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1150 - acc: 0.9527 - val_loss: 0.2991 - val_acc: 0.8989\n",
      "Epoch 29/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1148 - acc: 0.9529 - val_loss: 0.3032 - val_acc: 0.9033\n",
      "Epoch 30/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1098 - acc: 0.9544 - val_loss: 0.2918 - val_acc: 0.9013\n",
      "Epoch 31/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1068 - acc: 0.9554 - val_loss: 0.2952 - val_acc: 0.8985\n",
      "Epoch 32/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1245 - acc: 0.9455 - val_loss: 0.3030 - val_acc: 0.9009\n",
      "Epoch 33/50\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1100 - acc: 0.9547 - val_loss: 0.3115 - val_acc: 0.8965\n",
      "Epoch 34/50\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1109 - acc: 0.9551 - val_loss: 0.3011 - val_acc: 0.9009\n",
      "Epoch 35/50\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1062 - acc: 0.9554 - val_loss: 0.2854 - val_acc: 0.9030\n",
      "Epoch 36/50\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1059 - acc: 0.9555 - val_loss: 0.2810 - val_acc: 0.8996\n",
      "Epoch 37/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1051 - acc: 0.9551 - val_loss: 0.2669 - val_acc: 0.9057\n",
      "Epoch 38/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1040 - acc: 0.9555 - val_loss: 0.2757 - val_acc: 0.9108\n",
      "Epoch 39/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1021 - acc: 0.9566 - val_loss: 0.3057 - val_acc: 0.9060\n",
      "Epoch 40/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1073 - acc: 0.9561 - val_loss: 0.3216 - val_acc: 0.8985\n",
      "Epoch 41/50\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1020 - acc: 0.9567 - val_loss: 0.3532 - val_acc: 0.9002\n",
      "Epoch 42/50\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.1019 - acc: 0.9561 - val_loss: 0.2941 - val_acc: 0.9050\n",
      "Epoch 43/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.0984 - acc: 0.9588 - val_loss: 0.3199 - val_acc: 0.9104\n",
      "Epoch 44/50\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1060 - acc: 0.9562 - val_loss: 0.3198 - val_acc: 0.9114\n",
      "Epoch 45/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.0994 - acc: 0.9554 - val_loss: 0.2976 - val_acc: 0.9118\n",
      "Epoch 46/50\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.0975 - acc: 0.9582 - val_loss: 0.3030 - val_acc: 0.9097\n",
      "Epoch 47/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.0956 - acc: 0.9587 - val_loss: 0.2551 - val_acc: 0.9216\n",
      "Epoch 48/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.0995 - acc: 0.9561 - val_loss: 0.2765 - val_acc: 0.9094\n",
      "Epoch 49/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.0950 - acc: 0.9588 - val_loss: 0.2932 - val_acc: 0.9145\n",
      "Epoch 50/50\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.0976 - acc: 0.9582 - val_loss: 0.2320 - val_acc: 0.9148\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model2.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "model2.save('HAR_advanced.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I intentionally tried to make the model overfit(train for more epochs...) as it is a general practice to over-fit and  than regularize....(but it seems i could as well run for more epochs).95.88 (better than handcrafted features) is cool enough .....\n",
    "## AS FAR AS ARCHITECTURE IS CONSIDERED I WON'T PRETEND TO HAVE SOME DEEP INSIGHTNS...IT'S ALMOST ALWAYS LARGELY BASED ON TRYING COUPLE OF SENSIBLE ONES.....BUT AS FAR AS TRAINING METHOD IS CONCERNED\n",
    "## -> [CYCLICAL LEARNING RATES - LESLIE SMITH](https://arxiv.org/abs/1506.01186)\n",
    "## ->[SUPER CONVERGANCE](https://arxiv.org/abs/1708.07120)\n",
    "\n",
    "HERE I WON'T GO INTO THE DETAILS BUT IN LATER NOTEBOOK I WOULD TRY TO IMPLEMENT THEM(AS THEY ARE NOT READILY \n",
    "AVAILABLE) IN PYTORCH....BY FAR THESE ARE THE MOST AMAZING(PRACTICAL) PAPERS I HAVE READ....IT'S CRAZY THAT ALMOST NOBODY SEEMS TO TALK ABOUT IT .... THOSE TECHNIQUES COMBINED WITH FEW OTHER HACKS (I WILL SHARE THEM IN LATER NOTEBOOKS) HAVE GIVEN ME BETTER RESULTS(IMDB DATASET) THEN ANY CURRENTLY PUBLISHED ONES.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f3ec72fbf98>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "True                                                                         \n",
      "LAYING                 512        0        25        0                   0   \n",
      "SITTING                  3      410        75        0                   0   \n",
      "STANDING                 0       87       445        0                   0   \n",
      "WALKING                  0        0         0      481                   2   \n",
      "WALKING_DOWNSTAIRS       0        0         0        0                 382   \n",
      "WALKING_UPSTAIRS         0        0         0        2                  18   \n",
      "\n",
      "Pred                WALKING_UPSTAIRS  \n",
      "True                                  \n",
      "LAYING                             0  \n",
      "SITTING                            3  \n",
      "STANDING                           0  \n",
      "WALKING                           13  \n",
      "WALKING_DOWNSTAIRS                38  \n",
      "WALKING_UPSTAIRS                 451  \n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3087582236972612, 0.9097387173396675]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
